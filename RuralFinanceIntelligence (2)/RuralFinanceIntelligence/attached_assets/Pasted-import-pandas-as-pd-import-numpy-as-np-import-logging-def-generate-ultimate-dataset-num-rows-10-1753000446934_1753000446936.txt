import pandas as pd
import numpy as np
import logging

def generate_ultimate_dataset(num_rows=10000000):
    """
    Generates the final, professional-grade dataset with 10 million data points.
    This version uses correlated micro-personas and a sophisticated risk-profiling matrix
    based on a synthesis of financial reports and investment frameworks.
    """
    logging.info(f"--- Generating {num_rows} ultimate research-backed user profiles ---")
    
    # --- 1. Define Realistic Micro-Personas & State-Level Data (from Research) ---
    personas_config = {
        'Marginal_Farmer_Bihar': {'prevalence': 0.15, 'age_range': (25, 70), 'income_range': (4000, 12000), 'edu_dist': [0.6, 0.35, 0.05, 0.0]},
        'Irrigated_Farmer_Punjab': {'prevalence': 0.10, 'age_range': (30, 65), 'income_range': (20000, 60000), 'edu_dist': [0.2, 0.5, 0.25, 0.05]},
        'Rural_Laborer_UP': {'prevalence': 0.20, 'age_range': (18, 60), 'income_range': (5000, 15000), 'edu_dist': [0.7, 0.25, 0.05, 0.0]},
        'Small_Town_Trader_MH': {'prevalence': 0.20, 'age_range': (25, 60), 'income_range': (25000, 100000), 'edu_dist': [0.1, 0.3, 0.5, 0.1]},
        'Urban_Gig_Worker_KA': {'prevalence': 0.15, 'age_range': (20, 40), 'income_range': (20000, 90000), 'edu_dist': [0.0, 0.1, 0.6, 0.3]},
        'Salaried_Formal_TN': {'prevalence': 0.10, 'age_range': (24, 58), 'income_range': (40000, 300000), 'edu_dist': [0.0, 0.05, 0.4, 0.55]},
        'Retired_Pensioner': {'prevalence': 0.10, 'age_range': (60, 85), 'income_range': (8000, 50000), 'edu_dist': [0.2, 0.4, 0.3, 0.1]}
    }
    
    # Generate profiles based on persona prevalence
    profile_list = []
    for persona, props in personas_config.items():
        count = int(num_rows * props['prevalence'])
        profile = {
            'Persona': np.full(count, persona),
            'Age': np.random.randint(props['age_range'][0], props['age_range'][1], size=count),
            'Monthly_Income': np.random.randint(props['income_range'][0], props['income_range'][1], size=count),
            'Education_Level': np.random.choice(['Illiterate', 'Primary', 'Secondary', 'Graduate'], count, p=props['edu_dist']),
        }
        profile_list.append(pd.DataFrame(profile))
        
    df = pd.concat(profile_list, ignore_index=True).sample(frac=1).reset_index(drop=True)

    # --- 2. Generate Deeply Correlated Features ---
    df['Dependents'] = df['Age'].apply(lambda age: np.random.randint(0, 2) if age < 25 or age > 60 else np.random.randint(1, 6))
    df['Debt_Amount'] = (df['Monthly_Income'] * 12 * np.random.uniform(0.1, 2.5, size=num_rows)).astype(int)
    df['Debt_to_Income_Ratio'] = np.clip(df['Debt_Amount'] / (df['Monthly_Income'] * 12 + 1), 0, 5)
    df['Financial_Literacy_Score'] = df['Education_Level'].map({'Illiterate': 1, 'Primary': 3, 'Secondary': 6, 'Graduate': 9}) + np.random.randint(0, 2, size=num_rows)
    df['Time_Horizon_Years'] = np.random.randint(1, 40, size=num_rows)
    df['Risk_Tolerance'] = df.apply(lambda row: np.random.choice(['Low', 'Medium']) if row['Financial_Literacy_Score'] < 5 else np.random.choice(['Low', 'Medium', 'High']), axis=1)

    # --- 3. The "Expert Brain": Professional Portfolio Allocation ---
    logging.info("Assigning portfolios using professional risk-profiling matrix...")
    
    # Baseline portfolios informed by RBI and global asset allocation models.
    PORTFOLIO_MATRIX = {
        ('Low_Capacity', 'Low_Tolerance'):   {'Equity_Large_Cap': 0.00, 'Equity_Mid_Cap': 0.00, 'Debt_Govt_Scheme': 0.50, 'Debt_FD': 0.30, 'Gold': 0.15, 'Cash': 0.05},
        ('Med_Capacity', 'Low_Tolerance'):   {'Equity_Large_Cap': 0.10, 'Equity_Mid_Cap': 0.05, 'Debt_Govt_Scheme': 0.40, 'Debt_FD': 0.30, 'Gold': 0.10, 'Cash': 0.05},
        ('Med_Capacity', 'Med_Tolerance'):   {'Equity_Large_Cap': 0.25, 'Equity_Mid_Cap': 0.10, 'Debt_Govt_Scheme': 0.25, 'Debt_FD': 0.25, 'Gold': 0.10, 'Cash': 0.05},
        ('High_Capacity', 'Med_Tolerance'):  {'Equity_Large_Cap': 0.40, 'Equity_Mid_Cap': 0.15, 'Debt_Govt_Scheme': 0.15, 'Debt_FD': 0.15, 'Gold': 0.10, 'Cash': 0.05},
        ('High_Capacity', 'High_Tolerance'): {'Equity_Large_Cap': 0.50, 'Equity_Mid_Cap': 0.25, 'Debt_Govt_Scheme': 0.05, 'Debt_FD': 0.10, 'Gold': 0.05, 'Cash': 0.05}
    }

    def assign_portfolio(row):
        # Step A: Determine Risk Capacity (Ability to take risk, based on financial stability)
        capacity_score = 0
        if row['Monthly_Income'] > 40000: capacity_score += 2
        elif row['Monthly_Income'] > 15000: capacity_score += 1
        if row['Debt_to_Income_Ratio'] < 0.4: capacity_score += 1
        if row['Age'] < 45: capacity_score += 1
        
        if capacity_score <= 1: risk_capacity = 'Low_Capacity'
        elif capacity_score <= 3: risk_capacity = 'Med_Capacity'
        else: risk_capacity = 'High_Capacity'
        
        # Step B: Determine Risk Tolerance (Willingness to take risk)
        if row['Risk_Tolerance'] == 'Low': risk_tolerance = 'Low_Tolerance'
        elif row['Risk_Tolerance'] == 'Medium': risk_tolerance = 'Med_Tolerance'
        else: risk_tolerance = 'High_Tolerance'

        # Step C: Assign portfolio from the professional matrix
        portfolio_key = (risk_capacity, risk_tolerance)
        if portfolio_key not in PORTFOLIO_MATRIX:
            portfolio_key = ('Med_Capacity', 'Med_Tolerance') # A safe default

        allocations = PORTFOLIO_MATRIX[portfolio_key].copy()
        
        return pd.Series({f"{k}_Allocation": v for k, v in allocations.items()})

    portfolio_df = df.apply(assign_portfolio, axis=1)
    final_df = pd.concat([df, portfolio_df], axis=1)
    
    return final_df

if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    master_dataset = generate_ultimate_dataset(num_rows=10000000)
    
    output_filename = 'ultimate_training_data.csv'
    master_dataset.to_csv(output_filename, index=False)
    
    logging.info(f"âœ… Success! Ultimate professional dataset with {len(master_dataset)} rows is ready.")
    logging.info(f"File saved as: '{output_filename}'")
    
    print("\n--- Preview of the Ultimate Professional Dataset ---")
    print(master_dataset.head())